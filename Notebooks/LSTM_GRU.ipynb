{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdK4YXUcV75z",
        "outputId": "70089012-d0fa-4bff-c9fb-48be63802fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchaudio 0.10.0+cu113\n",
            "Uninstalling torchaudio-0.10.0+cu113:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/torchaudio-0.10.0+cu113.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torchaudio/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled torchaudio-0.10.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fMjoXqYaJBN",
        "outputId": "713ff1e0-2d51-4e6a-e31b-f29fc9e0b212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-25 02:44:17--  https://ai-data.obs.ru-moscow-1.hc.sbercloud.ru/%D0%9C%D0%9E_%D0%BD%D0%B0_%D0%B3%D1%80%D0%B0%D1%84%D0%B0%D1%85.zip\n",
            "Resolving ai-data.obs.ru-moscow-1.hc.sbercloud.ru (ai-data.obs.ru-moscow-1.hc.sbercloud.ru)... 46.243.206.34, 46.243.206.35\n",
            "Connecting to ai-data.obs.ru-moscow-1.hc.sbercloud.ru (ai-data.obs.ru-moscow-1.hc.sbercloud.ru)|46.243.206.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3399326506 (3.2G) [application/zip]\n",
            "Saving to: ‘МО_на_графах.zip’\n",
            "\n",
            "МО_на_графах.zip    100%[===================>]   3.17G  69.9MB/s    in 48s     \n",
            "\n",
            "2022-09-25 02:45:05 (66.9 MB/s) - ‘МО_на_графах.zip’ saved [3399326506/3399326506]\n",
            "\n",
            "Archive:  МО_на_графах.zip\n",
            "  inflating: ВК/hackathon_mos_sep2022.zip  \n",
            "error: invalid zip file with overlapped components (possible zip bomb)\n",
            "Archive:  /content/ВК/hackathon_mos_sep2022.zip\n",
            " extracting: ./zippeddataset/FINAL_ALL_SEQUENCES_TRAINTEST.tsv.zip  \n",
            " extracting: ./zippeddataset/FINAL_FEATURES_FRIENDS.tsv.zip  \n",
            " extracting: ./zippeddataset/FINAL_FEATURES_TRAINTEST.tsv.zip  \n",
            " extracting: ./zippeddataset/FINAL_SEQUENCES_MATRIX.tsv.zip  \n",
            " extracting: ./zippeddataset/FINAL_TARGETS_DATES_TRAINTEST.tsv.zip  \n",
            "Archive:  /content/zippeddataset/FINAL_ALL_SEQUENCES_TRAINTEST.tsv.zip\n",
            "  inflating: ./dataset/FINAL_ALL_SEQUENCES_TRAINTEST.tsv  \n",
            "  inflating: ./dataset/__MACOSX/._FINAL_ALL_SEQUENCES_TRAINTEST.tsv  \n",
            "Archive:  /content/zippeddataset/FINAL_FEATURES_FRIENDS.tsv.zip\n",
            "  inflating: ./dataset/FINAL_FEATURES_FRIENDS.tsv  \n",
            "  inflating: ./dataset/__MACOSX/._FINAL_FEATURES_FRIENDS.tsv  \n",
            "Archive:  /content/zippeddataset/FINAL_FEATURES_TRAINTEST.tsv.zip\n",
            "  inflating: ./dataset/FINAL_FEATURES_TRAINTEST.tsv  \n",
            "  inflating: ./dataset/__MACOSX/._FINAL_FEATURES_TRAINTEST.tsv  \n",
            "Archive:  /content/zippeddataset/FINAL_SEQUENCES_MATRIX.tsv.zip\n",
            "  inflating: ./dataset/FINAL_SEQUENCES_MATRIX.tsv  \n",
            "  inflating: ./dataset/__MACOSX/._FINAL_SEQUENCES_MATRIX.tsv  \n",
            "Archive:  /content/zippeddataset/FINAL_TARGETS_DATES_TRAINTEST.tsv.zip\n",
            "  inflating: ./dataset/FINAL_TARGETS_DATES_TRAINTEST.tsv  \n",
            "  inflating: ./dataset/__MACOSX/._FINAL_TARGETS_DATES_TRAINTEST.tsv  \n"
          ]
        }
      ],
      "source": [
        "!wget https://ai-data.obs.ru-moscow-1.hc.sbercloud.ru/МО_на_графах.zip\n",
        "!unzip МО_на_графах.zip\n",
        "!unzip /content/ВК/hackathon_mos_sep2022.zip -d ./zippeddataset/\n",
        "!unzip /content/zippeddataset/FINAL_ALL_SEQUENCES_TRAINTEST.tsv.zip -d ./dataset/\n",
        "!unzip /content/zippeddataset/FINAL_FEATURES_FRIENDS.tsv.zip -d ./dataset/\n",
        "!unzip /content/zippeddataset/FINAL_FEATURES_TRAINTEST.tsv.zip -d ./dataset/\n",
        "!unzip /content/zippeddataset/FINAL_SEQUENCES_MATRIX.tsv.zip -d ./dataset/\n",
        "!unzip /content/zippeddataset/FINAL_TARGETS_DATES_TRAINTEST.tsv.zip -d ./dataset/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "OCERyjeULsD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a5b5fa-28a1-4d0c-dad8-728513e5792e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 14.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 12.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=fbdfebef8f282d50c05cb35d9899fd12bfba2e930944da1b0624f540ee17afc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.1.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import torch\n",
        "from torch_geometric.data import Dataset, download_url, Data\n",
        "\n",
        "\n",
        "class NodeDataset(Dataset):\n",
        "    def __init__(self, count_rows = None, root = \"/content/dataset/\", transform=None, pre_transform=None, pre_filter=None):\n",
        "        self.sequence_predict = []\n",
        "        self.files_data = []\n",
        "        self.count_rows = count_rows\n",
        "        \n",
        "        super().__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['FINAL_ALL_SEQUENCES_TRAINTEST.tsv', 'FINAL_FEATURES_FRIENDS.tsv', 'FINAL_FEATURES_TRAINTEST.tsv', 'FINAL_TARGETS_DATES_TRAINTEST.tsv']\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return self.files_data\n",
        "    \n",
        "    def download(self):\n",
        "        return ['FINAL_ALL_SEQUENCES_TRAINTEST.tsv', 'FINAL_FEATURES_FRIENDS.tsv', 'FINAL_FEATURES_TRAINTEST.tsv', 'FINAL_TARGETS_DATES_TRAINTEST.tsv']\n",
        "    \n",
        "    def process_sequence(self, sequence, latent_space = 50):\n",
        "        processed_sequence = torch.from_numpy(np.zeros((latent_space)))\n",
        "        series = pd.rear_csv('dict_hashes.csv')\n",
        "        dict_hashes = series.to_dict()\n",
        "        series = pd.rear_csv('rand_v.csv')\n",
        "        rand_v = series.to_dict()\n",
        "\n",
        "        processed_vec = [[rand_v[dict_hashes[j]][0] for j in row] for row in a.values()]\n",
        "        token_start = 10\n",
        "        token_end = -10\n",
        "        tokenized_vec = [[token_start] + i + [token_end] for i in processed_vec]\n",
        "\n",
        "        latent_length = 100\n",
        "\n",
        "        post_processed = np.array([np.concatenate((np.zeros((latent_length)), i))[-latent_length:] for i in tokenized_vec])\n",
        "        PATH = ''\n",
        "        lstm = torch.load(PATH)\n",
        "        with torch.no_grad():\n",
        "          tag_scores = lstm(torch.from_numpy(post_processed).float())\n",
        "        self.sequence_predict.append(tag_scores[1])\n",
        "        \n",
        "    \n",
        "    def process(self):\n",
        "        # последовательности пользовательских состояний для пользователей из основной выборки\n",
        "        ALL_SEQUENCES_TRAINTEST = self.root + '/FINAL_ALL_SEQUENCES_TRAINTEST.tsv'\n",
        "\n",
        "        # связь между пользователем из основной выборки и всеми его друзьями (граф) + те же самые два домена признаков для друзей\n",
        "        FEATURES_FRIENDS = self.root + '/FINAL_FEATURES_FRIENDS.tsv'\n",
        "\n",
        "        # 2 домена вещественных признаков\n",
        "        FEATURES_TRAINTEST = self.root + '/FINAL_FEATURES_TRAINTEST.tsv'\n",
        "\n",
        "        # метки классов, ретродаты и идентификаторы тестовых объектов\n",
        "        TARGETS_DATES_TRAINTEST = self.root + '/FINAL_TARGETS_DATES_TRAINTEST.tsv'\n",
        "        \n",
        "        ####DATAFRAMES####\n",
        "        \n",
        "        client_sequences = pd.read_csv(ALL_SEQUENCES_TRAINTEST,sep='\\t')\n",
        "        \n",
        "        targets = pd.read_csv(TARGETS_DATES_TRAINTEST,sep='\\t')\n",
        "        \n",
        "        features = pd.read_csv(FEATURES_TRAINTEST,sep='\\t')\n",
        "        \n",
        "        ####Process graph####\n",
        "                \n",
        "        client_set = set()    \n",
        "        \n",
        "        \n",
        "        dataframe_friends = pd.DataFrame()\n",
        "        chunksize = 100\n",
        "        rows = 0\n",
        "        idx = 0\n",
        "        for friends_chunk in pd.read_csv(FEATURES_FRIENDS, sep='\\t', chunksize=chunksize, nrows=self.count_rows):\n",
        "            for row in friends_chunk.iloc():\n",
        "                rows += 1;\n",
        "                if rows % 10000 == 0:\n",
        "                    if self.count_rows == None: print(rows)\n",
        "                    else:  print(float(rows) / float(self.count_rows) * 100, \"%\")\n",
        "                if row['CLIENT_ID'] in client_set:\n",
        "                    dataframe_friends = pd.concat([dataframe_friends, row], axis=1)\n",
        "                else:\n",
        "                    if len(client_set) > 0:\n",
        "                        client_feature = torch.tensor([features[features['CLIENT_ID'] == row['CLIENT_ID']].values[0][1:]])\n",
        "                        friend_feature = torch.tensor(dataframe_friends.values)[1:-1]\n",
        "                        \n",
        "                        if len(friend_feature.shape) == 1: friend_feature = friend_feature.reshape(1014, -1)\n",
        "                        \n",
        "                        friend_feature = friend_feature.transpose(0, 1)\n",
        "                        \n",
        "                        X = torch.cat((client_feature, friend_feature))\n",
        "                        \n",
        "                        edges = torch.from_numpy(np.concatenate((np.zeros((len(friend_feature))), np.arange(1,len(friend_feature) + 1))).reshape((2, -1))).int()\n",
        "                        edge_attrs = torch.tensor(dataframe_friends.values)[-1] % 100\n",
        "                                                \n",
        "                        sequence = client_sequences[client_sequences['CLIENT_ID'] == row['CLIENT_ID']]['SEQUENCE'].values\n",
        "                        self.process_sequence(sequence)\n",
        "                        \n",
        "                        predict = targets[targets['CLIENT_ID'] == row['CLIENT_ID']]['TARGET'].values\n",
        "                        \n",
        "                        if predict[0] == 'test': predict[0] = -1\n",
        "                        else: predict[0] = float(predict[0])\n",
        "                        \n",
        "                        predict = torch.from_numpy(np.array(predict).astype(float))\n",
        "                        \n",
        "                        data = Data(\n",
        "                            x = X,\n",
        "                            edge_index = edges,\n",
        "                            edge_attr = edge_attrs,\n",
        "                            y = predict\n",
        "                        )\n",
        "                        \n",
        "                        torch.save(data, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "                        self.files_data.append(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "                        idx += 1\n",
        "                        \n",
        "                    client_set.add(row['CLIENT_ID'])\n",
        "                    dataframe_friends = row\n",
        "                    \n",
        "            \n",
        "\n",
        "    def len(self):\n",
        "        return len(self.files_data)\n",
        "\n",
        "    def get(self, idx):\n",
        "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "        return data\n",
        "    \n",
        "ds = NodeDataset(500)"
      ],
      "metadata": {
        "id": "CMNDiICtKIAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4962de5d-39e5-49aa-866a-6c0704b8de86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkdOgiXYtmm4",
        "outputId": "e1db9bac-06c8-49f2-e763-83a198b03c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[81, 1014], edge_index=[2, 80], edge_attr=[80], y=[1])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
      ],
      "metadata": {
        "id": "wVbyvVdcUNX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KwiPn1__uvSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ldj13YnZH5ag",
        "outputId": "dfe5abe7-9b51-4f87-eb8e-51d4d75e9f83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "torch.manual_seed(1)\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCpHWrV8d-Gl"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/dataset/FINAL_SEQUENCES_MATRIX.tsv\", sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxCKOze7w396"
      },
      "outputs": [],
      "source": [
        "a = {}\n",
        "\n",
        "set_of_hash = set()\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    temp = [i[1:-1] for i in row['SESSIONS_SEQUENCES'][1:-1].split(\", \")]\n",
        "\n",
        "    for i in temp: \n",
        "      set_of_hash.add(i)\n",
        "\n",
        "    a[index] = temp\n",
        "\n",
        "n_latent = 1\n",
        "\n",
        "rand_v = np.random.randn(len(set_of_hash), n_latent)\n",
        "\n",
        "dict_hashes = {}\n",
        "\n",
        "list_of_hash= list(set_of_hash)\n",
        "\n",
        "for i, j in zip(range(len(set_of_hash)), list_of_hash):\n",
        "   dict_hashes[j] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMi9ok1Fw4Ad"
      },
      "outputs": [],
      "source": [
        "# processed_vec = [[rand_v[dict_hashes[j]][0] for j in row] for row in a.values()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(rand_v)"
      ],
      "metadata": {
        "id": "clIJgIJe9Hgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBjFkYWYEDVx"
      },
      "outputs": [],
      "source": [
        "# series = pd.Series(dict_hashes)\n",
        "# series.to_csv('dict_hashes.csv', index=False)\n",
        "# series = pd.Series(list(rand_v))\n",
        "# series.to_csv('rand_v.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBmsjXdmw4DB"
      },
      "outputs": [],
      "source": [
        "# series = pd.rear_csv('dict_hashes.csv')\n",
        "# dict_hashes = series.to_dict()\n",
        "# series = pd.rear_csv('rand_v.csv')\n",
        "# rand_v = series.to_dict()\n",
        "\n",
        "processed_vec = [[rand_v[dict_hashes[j]][0] for j in row] for row in a.values()]\n",
        "token_start = 10\n",
        "token_end = -10\n",
        "tokenized_vec = [[token_start] + i + [token_end] for i in processed_vec]\n",
        "\n",
        "latent_length = 100\n",
        "\n",
        "post_processed = np.array([np.concatenate((np.zeros((latent_length)), i))[-latent_length:] for i in tokenized_vec])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del data\n",
        "del processed_vec\n",
        "del tokenized_vec"
      ],
      "metadata": {
        "id": "1IRKNhEy7amG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frnjmHWVw4FX"
      },
      "outputs": [],
      "source": [
        "train = data_utils.TensorDataset(torch.from_numpy(post_processed).float())\n",
        "train_loader = data_utils.DataLoader(train, batch_size=34, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSNUQMdHIHcI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CreditsRNN(nn.Module):\n",
        "    def __init__(self, seq_vec_size, rnn_units=2, top_classifier_units=2, num_out = 100):\n",
        "        super(CreditsRNN, self).__init__()\n",
        "        self.seq_vec_size = seq_vec_size\n",
        "                        \n",
        "        self._gru = nn.GRU(input_size=seq_vec_size,\n",
        "                             hidden_size=rnn_units, batch_first=True, bidirectional=False)\n",
        "        \n",
        "        \n",
        "        self._hidden_size = rnn_units\n",
        "\n",
        "        # self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
        "        self._top_classifier = nn.Linear(in_features=rnn_units, out_features=100)\n",
        "        # self._intermediate_activation = nn.ReLU()\n",
        "        # self._head = nn.Linear(in_features=top_classifier_units, out_features=num_out)\n",
        "    \n",
        "    def forward(self, seq_vec):\n",
        "\n",
        "        \n",
        "        _, last_hidden = self._gru(seq_vec)\n",
        "                                \n",
        "        classification_hidden = self._top_classifier(last_hidden)\n",
        "        # activation = self._intermediate_activation(classification_hidden)\n",
        "        # raw_output = self._head(activation)\n",
        "        return classification_hidden, last_hidden\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-906NVv6Qjfh",
        "outputId": "5c8d8160-1584-4b5f-bd1f-be0e9a91cfb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CreditsRNN(\n",
              "  (_gru): GRU(100, 50, batch_first=True)\n",
              "  (_top_classifier): Linear(in_features=50, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# RNN Длина последовательности (seq_vec_size), латентное представление(rnn_units)\n",
        "# Liner classification_hidden - выход, rnn_units - вход\n",
        "model = CreditsRNN(seq_vec_size = 500, rnn_units = 50, num_out = 100)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHFiVkS8JFQr"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(lr=1e-3, params=model.parameters())\n",
        "loss_function = torch.nn.L1Loss()\n",
        "best_loss = 9999999\n",
        "for epoch in range(500):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    norm = 0\n",
        "    mae = 0\n",
        "    val_data = []\n",
        "    val_error = 0\n",
        "    for x_batch in train_loader:\n",
        "        norm +=1\n",
        "        model.zero_grad()\n",
        "        sequense, val = train_test_split(x_batch[0], test_size=0.1, random_state=42)\n",
        "        val_data.append(val)\n",
        "        tag_scores = model(sequense)\n",
        "        loss = loss_function(tag_scores[0], sequense)\n",
        "        mae += loss.tolist()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    with torch.no_grad():\n",
        "       for val in val_data:\n",
        "        tag_scores = model(val)\n",
        "        loss = loss_function(tag_scores[0], val)\n",
        "        val_error += loss.tolist()\n",
        "    if best_loss<val_error/norm:\n",
        "      best_loss = val_error/norm\n",
        "      torch.save(model.state_dict(), '/content/sample_data/model_dict')\n",
        "      torch.save(model, '/content/sample_data/model')\n",
        "    \n",
        "    print('Epoha=', epoch+1,' Train loss=', mae/norm, ' Val loss=', val_error/norm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CreditsLSTM(nn.Module):\n",
        "    def __init__(self, seq_vec_size, rnn_units, top_classifier_units=100, num_out = 100):\n",
        "        super(CreditsLSTM, self).__init__()\n",
        "        self.seq_vec_size = seq_vec_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=seq_vec_size,\n",
        "                             hidden_size=rnn_units, batch_first=True, bidirectional=False)\n",
        "                        \n",
        "        # self._gru = nn.GRU(input_size=seq_vec_size,\n",
        "        #                      hidden_size=rnn_units, batch_first=True, bidirectional=False)\n",
        "        \n",
        "        \n",
        "        self._hidden_size = rnn_units\n",
        "\n",
        "        # self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
        "        self._top_classifier = nn.Linear(in_features=rnn_units, out_features=100)\n",
        "        # self._intermediate_activation = nn.ReLU()\n",
        "        # self._head = nn.Linear(in_features=top_classifier_units, out_features=num_out)\n",
        "    \n",
        "    def forward(self, seq_vec):\n",
        "\n",
        "        \n",
        "        _, last_hidden = self.lstm(seq_vec)\n",
        "        # print(last_hidden)                                \n",
        "        classification_hidden = self._top_classifier(last_hidden[0])\n",
        "        # activation = self._intermediate_activation(classification_hidden)\n",
        "        # raw_output = self._head(activation)\n",
        "        return classification_hidden, last_hidden"
      ],
      "metadata": {
        "id": "tgpygtRtEd-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = CreditsLSTM(seq_vec_size = 100, rnn_units = 50, num_out = 100)\n",
        "lstm"
      ],
      "metadata": {
        "id": "dOJ5VSNZEw2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfe2b25-16a4-458c-a4d6-c5d3f180f01e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CreditsLSTM(\n",
              "  (lstm): LSTM(100, 50, batch_first=True)\n",
              "  (_top_classifier): Linear(in_features=50, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(lr=1e-3, params=lstm.parameters())\n",
        "loss_function = torch.nn.L1Loss()\n",
        "best_loss = 9999999\n",
        "for epoch in range(2):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    norm = 0\n",
        "    mae = 0\n",
        "    val_data = []\n",
        "    val_error = 0\n",
        "    for x_batch in train_loader:\n",
        "        norm +=1\n",
        "        lstm.zero_grad()\n",
        "        sequense, val = train_test_split(x_batch[0], test_size=0.1, random_state=42)\n",
        "        val_data.append(val)\n",
        "        tag_scores = lstm(sequense)\n",
        "        loss = loss_function(tag_scores[0], sequense)\n",
        "        mae += loss.tolist()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    with torch.no_grad():\n",
        "       for val in val_data:\n",
        "        tag_scores = lstm(val)\n",
        "        loss = loss_function(tag_scores[0], val)\n",
        "        val_error += loss.tolist()\n",
        "    if best_loss>val_error/norm:\n",
        "      best_loss = val_error/norm\n",
        "      torch.save(lstm.state_dict(), '/content/sample_data/lstm_dict.pth')\n",
        "      torch.save(lstm, '/content/sample_data/lstm.pth')\n",
        "    \n",
        "    print('Epoha=', epoch+1,' Train loss=', mae/norm, ' Val loss=', val_error/norm)"
      ],
      "metadata": {
        "id": "Dw-lkAdeFIAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1644550-0675-4886-b6a3-40e9645fac2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([30, 100])) that is different to the input size (torch.Size([1, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([4, 100])) that is different to the input size (torch.Size([1, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoha= 1  Train loss= 0.23622847775909375  Val loss= 0.23612689835044154\n",
            "Epoha= 2  Train loss= 0.23488073586930555  Val loss= 0.2495300196875443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(lstm.state_dict(), '/content/sample_data/lstm_dict.pth')\n",
        "torch.save(lstm, '/content/sample_data/lstm.pth')"
      ],
      "metadata": {
        "id": "9TNnHeQRyon0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class must be defined somewhere\n",
        "model = torch.load(PATH)\n",
        "model.eval()\n",
        "\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "ylHJdx885SC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y6ZszsBAKLm"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    inputs = torch.Tensor([0,1,0,1,0])\n",
        "    print(inputs.size(-1) )\n",
        "\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n",
        "    print(targets.shape)\n",
        "    loss_function(tag_scores, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPNe0Doa_QYz"
      },
      "outputs": [],
      "source": [
        "def eval_model(model: torch.nn.Module, dataset_val: List[str], batch_size: int = 32, device: torch.device = None) -> float:\n",
        "\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "    val_generator = batches_generator(dataset_val, batch_size=batch_size, shuffle=False,\n",
        "                                      device=device, is_train=True, output_format=\"torch\")\n",
        "\n",
        "    for batch in tqdm_notebook(val_generator, desc=\"Evaluating model\"):\n",
        "        targets.extend(batch[\"label\"].detach().cpu().numpy().flatten())\n",
        "        output = model(batch[\"features\"])\n",
        "        preds.extend(output.detach().cpu().numpy().flatten())\n",
        "\n",
        "    return roc_auc_score(targets, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWpnR0gTGCHY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmnl3qlet5yK"
      },
      "outputs": [],
      "source": [
        "# Define the pytorch model\n",
        "class torchLSTM(torch.nn.Module):\n",
        "    def __init__(self, n_features, seq_length):\n",
        "        super(torchLSTM, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.seq_len = seq_length\n",
        "        self.n_hidden = 100  # number of hidden states\n",
        "        self.n_layers = 1  # number of LSTM layers (stacked)\n",
        "\n",
        "        self.l_lstm = torch.nn.LSTM(input_size=n_features,\n",
        "                                    hidden_size=self.n_hidden,\n",
        "                                    num_layers=self.n_layers,\n",
        "                                    batch_first=True)\n",
        "        # according to pytorch docs LSTM output is\n",
        "        # (batch_size,seq_len, num_directions * hidden_size)\n",
        "        # when considering batch_first = True\n",
        "        self.l_linear = torch.nn.Linear(self.n_hidden * self.seq_len, 3)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # even with batch_first = True this remains same as docs\n",
        "        hidden_state = torch.zeros(self.n_layers, batch_size, self.n_hidden)\n",
        "        cell_state = torch.zeros(self.n_layers, batch_size, self.n_hidden)\n",
        "        self.hidden = (hidden_state, cell_state)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        lstm_out, self.hidden = self.l_lstm(x, self.hidden)\n",
        "        # lstm_out(with batch_first = True) is\n",
        "        # (batch_size,seq_len,num_directions * hidden_size)\n",
        "        # for following linear layer we want to keep batch_size dimension and merge rest\n",
        "        # .contiguous() -> solves tensor compatibility error\n",
        "        x = lstm_out.contiguous().view(batch_size, -1)\n",
        "        return self.l_linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWCJ33HvvFRD",
        "outputId": "2972357a-68ad-4b3f-cc74-2ed4431bce13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torchLSTM(\n",
              "  (l_lstm): LSTM(5, 100, batch_first=True)\n",
              "  (l_linear): Linear(in_features=500, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_test = torchLSTM(5,5)\n",
        "model_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmI6BcgZ4y9n"
      },
      "outputs": [],
      "source": [
        "inputs = torch.Tensor([[[0,1,0,1,0,1]],[[0,1,0,1,0,1]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEkGGej89L_Q",
        "outputId": "23f5c7fc-9a52-4fbb-d9c1-ccc6eca1f681"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1., 0., 1., 0., 1.])"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs[0,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXfnyFmMP9Gv"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data={'sequence': [[0,1,0,1,0,1],[0,1,0,1,0,1],[0,1,0,1,0,1],[0,1,0,1,0,1]]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "W6MA_ZonIPG9",
        "outputId": "573c8733-6bbc-4f33-fa6a-551c0e786812"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e49df910-b1ca-4a73-ac47-b23883c1c758\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence</th>\n",
              "      <th>train</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 1, 0, 1, 0, 1]</td>\n",
              "      <td>[0, 1, 0, 1, 0]</td>\n",
              "      <td>[1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0, 1, 0, 1, 0, 1]</td>\n",
              "      <td>[0, 1, 0, 1, 0]</td>\n",
              "      <td>[1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 1, 0, 1, 0, 1]</td>\n",
              "      <td>[0, 1, 0, 1, 0]</td>\n",
              "      <td>[1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0, 1, 0, 1, 0, 1]</td>\n",
              "      <td>[0, 1, 0, 1, 0]</td>\n",
              "      <td>[1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e49df910-b1ca-4a73-ac47-b23883c1c758')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e49df910-b1ca-4a73-ac47-b23883c1c758 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e49df910-b1ca-4a73-ac47-b23883c1c758');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             sequence            train target\n",
              "0  [0, 1, 0, 1, 0, 1]  [0, 1, 0, 1, 0]    [1]\n",
              "1  [0, 1, 0, 1, 0, 1]  [0, 1, 0, 1, 0]    [1]\n",
              "2  [0, 1, 0, 1, 0, 1]  [0, 1, 0, 1, 0]    [1]\n",
              "3  [0, 1, 0, 1, 0, 1]  [0, 1, 0, 1, 0]    [1]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def seq_data(df, num_pred):\n",
        "  df['sequence']\n",
        "  df['train'] = df['sequence'].apply(lambda x: x[:-num_pred])\n",
        "  df['target'] = df['sequence'].apply(lambda x: x[-num_pred:])\n",
        "  return df\n",
        "seq_data(df, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF4wKPtCQ9Dh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIARo-GNO_df"
      },
      "outputs": [],
      "source": [
        "def norm_vector(vec, target):\n",
        "  # print(target)\n",
        "  len_vec = len(vec)\n",
        "  target = vec[-1]\n",
        "  vec = torch.Tensor(vec)\n",
        "  vec = np.reshape(vec, (1,len_vec))\n",
        "  target = torch.Tensor([target])\n",
        "  # print(target)\n",
        "  target = np.reshape(target, (1,1))\n",
        "  # print(target)\n",
        "  return vec, target\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A_3rfhIj7CY"
      },
      "outputs": [],
      "source": [
        "los = torch.nn.L1Loss()\n",
        "los(torch.Tensor([1,97,6,6]),torch.Tensor([2,9,8,]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsOMGmtRJOUF"
      },
      "outputs": [],
      "source": [
        "for index, row in df.iterrows():\n",
        "  vec, target = norm_vector(row['train'], row['target'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3Skc_zNJFTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-fvvjuHGZmB",
        "outputId": "57d7b761-7d38-4e4a-803d-292b382e64ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1., 0., 1., 0., 1.])"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec = [0,1,0,1,0,1]\n",
        "len_vec = len(vec)\n",
        "target = vec[-1]\n",
        "b = torch.Tensor(b)\n",
        "b = np.reshape(b, (1,6))\n",
        "target = torch.Tensor(target)\n",
        "target = np.reshape(target, (1,6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjEK0ULmD7_J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKX8BET_8Aov"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model: torch.nn.Module, optimizer: torch.optim.Optimizer,\n",
        "                batch_size: int = 64, print_loss_every_n_batches: int = 500):\n",
        "  \n",
        "    model.train()\n",
        "    loss_function = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "    losses = torch.LongTensor()\n",
        "    samples_counter = 0\n",
        "    train_generator = batches_generator(dataset_train, batch_size=batch_size, shuffle=shuffle,\n",
        "                                        device=device, is_train=True, output_format=\"torch\")\n",
        "\n",
        "    for num_batch, batch in tqdm_notebook(enumerate(train_generator, start=1), desc=\"Training\"):\n",
        "        output = torch.flatten(model(batch[\"features\"]))\n",
        "        batch_loss = loss_function(output, batch[\"label\"].float())\n",
        "        batch_loss.mean().backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        samples_counter += batch_loss.size(0)\n",
        "\n",
        "        losses = torch.cat([losses, batch_loss], dim=0)\n",
        "        if num_batch % print_loss_every_n_batches == 0:\n",
        "            print(f\"Batches {num_batch - print_loss_every_n_batches + 1} - {num_batch} loss:\"\n",
        "                  f\"{losses[-samples_counter:].mean()}\", end=\"\\r\")\n",
        "            samples_counter = 0\n",
        "\n",
        "    print(f\"Training loss after epoch: {losses.mean()}\", end=\"\\r\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DawUZoqVQjrE"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, size, hidden_dim, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tagset_size = tagset_size\n",
        "        # self.vec_seq = vec_seq\n",
        "        self.size = size\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(1, size, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(5, 1) # количество разных состояний\n",
        "\n",
        "    def forward(self, vec_seq):\n",
        "        # vec_seq = self.vec_seq\n",
        "        lstm_out, _ = self.lstm(vec_seq.view(self.size, 1, -1))\n",
        "        # print(lstm_out.view(self.size, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(self.size, -1))\n",
        "        print(tag_space.shape)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        print(tag_scores.shape)\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWvU6WgDQoCg",
        "outputId": "72cacca9-7b54-4ccb-d324-e5ecb07c0c7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMTagger(\n",
              "  (lstm): LSTM(1, 5, num_layers=3)\n",
              "  (hidden2tag): Linear(in_features=5, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LSTMTagger(size=5, hidden_dim=3, tagset_size=1)\n",
        "model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}